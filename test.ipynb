{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from main_library import *\n",
    "\n",
    "# # ################################\n",
    "# # ## data load / adjustements*\n",
    "# # extractDate = date_load()\n",
    "\n",
    "# # ### pour l'instant ne fonctionne pas\n",
    "# # ## demander à Eric de relancer la machine sur sandbox\n",
    "# # # get_call_info()\n",
    "\n",
    "# # # get_call_info_europa()\n",
    "\n",
    "# # proj = projects_load()\n",
    "# # proj_id_signed = proj.project_id.unique()\n",
    "\n",
    "# # prop = proposals_load()\n",
    "# # proj = proj_add_cols(prop, proj)\n",
    "\n",
    "# # stage_p =  ['REJECTED' ,'NO_MONEY' ,'MAIN', 'RESERVE', 'INELIGIBLE', 'WITHDRAWN', 'INADMISSIBLE', None]\n",
    "# # prop1 = proposals_status(prop, proj_id_signed, stage_p)  \n",
    "# # # np.save(\"data_files/applicants_columns.npy\", prop_cols)\n",
    "\n",
    "# # ###########################################\n",
    "# # # proposals fix\n",
    "# # # projects missing from proposals\n",
    "# # call_to_integrate, call_miss = proposals_id_missing(prop1, proj, extractDate)\n",
    "\n",
    "# # # project data missing in proposals if call already in proposals then add this\n",
    "# # proj1 = proj_id_miss_fixed(prop1, proj, call_to_integrate)\n",
    "# # call_miss = list(set(call_miss)-set(call_to_integrate))\n",
    "# # proj = proj.loc[~proj.callId.isin(call_miss)]\n",
    "\n",
    "# # # merge proj + prop\n",
    "# # print('### MERGED PROPOSALS/PROJECTS')\n",
    "# # if len(proj1)==0:\n",
    "# #     prop2=pd.concat([proj,prop1], ignore_index= True)\n",
    "# # else:\n",
    "# #     prop2 = pd.concat([prop1, proj1, proj], ignore_index = True)\n",
    "\n",
    "# # prop2 = prop2.loc[~((prop2.status_code=='REJECTED')&(prop2.stage=='successful'))]\n",
    "# # print(f\"- result - merged all: {len(prop2)},\\n{prop2[['stage','status_code']].value_counts()}\")\n",
    "\n",
    "# # merged = copy.deepcopy(prop2)\n",
    "# # merged = dates_year(merged)\n",
    "# # merged = strings_v(merged)\n",
    "# # merged = url_to_clean(merged)\n",
    "# # merged.mask(merged=='', inplace=True)\n",
    "# # merged = empty_str_to_none(merged)      \n",
    "# # merged.rename(columns={\n",
    "# #     'freekw':'free_keywords',\n",
    "# #     'callDeadlineDate':'call_deadline', \n",
    "# #     'callId':'call_id', \n",
    "# #     'submissionDate':'submission_date',\n",
    "# #     'startDate':'start_date',\n",
    "# #     'endDate':'end_date', \n",
    "# #     'ecSignatureDate':'signature_date'}, inplace=True)\n",
    "\n",
    "# # if any(merged.loc[merged.stage=='successful', 'project_id'].value_counts()[merged.loc[merged.stage=='successful', 'project_id'].value_counts()> 1]):\n",
    "# #     print(merged.loc[merged.stage=='successful', 'project_id'].value_counts()[merged.loc[merged.stage=='successful', 'project_id'].value_counts()> 1])\n",
    "\n",
    "# # merged = merged_panels(merged)\n",
    "# # merged = merged_topics(merged)\n",
    "# # merged = merged_actions(merged)\n",
    "\n",
    "# # # calls list\n",
    "# # calls = call(PATH_SOURCE+FRAMEWORK+'/')\n",
    "\n",
    "# # print(\"\\n### CALLS+MERGED\")\n",
    "# # if len(merged.loc[merged.call_id.isnull()])>0:\n",
    "# #         print(f\"1 - ATTENTION : manque des call_id: {merged.loc[merged.call_id.isnull(), 'project_id']}\")\n",
    "# # else:\n",
    "# #     call_id = merged[['call_id', 'call_deadline']].drop_duplicates()\n",
    "# #     print(f\"2 - CALL_ID de merged -> nb call+deadline: {len(call_id)}, nb call unique: {call_id.call_id.nunique()} \")\n",
    "\n",
    "# # calls = calls_to_check(calls, call_id)\n",
    "\n",
    "# # projects = projects_complete_cleaned(merged, extractDate)\n",
    "\n",
    "# # #############################################################\n",
    "# # ##### PARTICIPATIONS\n",
    "# # part = participants_load(proj)\n",
    "# # # conserve uniquement les projets présents dans proposals et applicants\n",
    "# # part = part.loc[part.project_id.isin(projects.project_id.unique())]\n",
    "# # print(f\"- size part hors proj manquant: {len(part)}\")\n",
    "# # part = part_role_type(part)\n",
    "# # part = erc_role(part, projects)\n",
    "\n",
    "# # #### APPLICANTS\n",
    "# # app = applicants_load(prop)\n",
    "# # # conserve uniquement les projets présents dans proposals et applicants\n",
    "# # app1 = app.loc[app.project_id.isin(projects.project_id.unique())] \n",
    "# # print(f\"- size app1 hors proj exclus: {len(app1)}\")\n",
    "\n",
    "# # app_missing_pid = projects.loc[(projects.stage=='evaluated')&(~projects.project_id.isin(app1.project_id.unique())), 'project_id'].unique()\n",
    "# # tmp = part[part.project_id.isin(app_missing_pid)]\n",
    "# # app1 = part_miss_app(tmp, app1)\n",
    "\n",
    "# # #redressement accelerator\n",
    "# # acc_folio = pd.read_csv(f\"{PATH_SOURCE}{FRAMEWORK}/eic_fund_portfolio.csv\", sep=';', dtype={'PROPOSAL_NBR':str})\n",
    "# # print(f\"size acc_folio: {len(acc_folio)}\")\n",
    "# # acc = (app1.loc[(app1.project_id.isin(acc_folio.PROPOSAL_NBR.unique()))&(app1.role=='Coordinator'),['project_id', 'role']]\n",
    "# #        .merge(acc_folio[['PROPOSAL_NBR','GRANT_REQUESTED']], how='inner', left_on='project_id', right_on='PROPOSAL_NBR')\n",
    "# #        .drop(columns='PROPOSAL_NBR'))\n",
    "# # print(f\"size acc: {len(acc)}\")\n",
    "# # app1 = app1.merge(acc, how='left', on=['project_id', 'role'])\n",
    "# # app1.loc[app1.requestedGrant.isnull(), 'requestedGrant'] = app1.GRANT_REQUESTED\n",
    "# # app1.drop(columns=['GRANT_REQUESTED'], inplace=True)\n",
    "\n",
    "# # app1 = app_role_type(app1)\n",
    "# # app1 = erc_role(app1, projects)\n",
    "\n",
    "# # del app\n",
    "\n",
    "# # ####\n",
    "# # # verification Etat des participations\n",
    "# # part = check_multiP_by_proj(part)\n",
    "# # app1 = check_multiA_by_proj(app1)\n",
    "\n",
    "\n",
    "# # ### STEP2\n",
    "# # lien = merged_partApp(app1, part)\n",
    "# # lien = nuts_lien(app1, part, lien)\n",
    "\n",
    "# # # ENTITIES\n",
    "# # entities = entities_load(lien)\n",
    "\n",
    "# # list_codeCountry = list(set(entities.countryCode.to_list()+lien.countryCode.to_list()))\n",
    "# # countries, countryCode_err = country_load(FRAMEWORK, list_codeCountry)\n",
    "# # if countryCode_err:\n",
    "# #     ccode=json.load(open(\"data_files/countryCode_match.json\"))\n",
    "# #     for i in ccode:\n",
    "# #         for k,v in i.items():\n",
    "# #             lien.loc[lien.countryCode==k, 'countryCode'] = v\n",
    "# #             entities.loc[entities.countryCode==k, 'countryCode'] = v\n",
    "# # cc_code = (countries[['countryCode', 'country_code_mapping']]\n",
    "# #            .rename(columns={'countryCode':'iso2', 'country_code_mapping':'iso3'})\n",
    "# #            .drop_duplicates())\n",
    "\n",
    "# # lien = lien.merge(cc_code, how='left', left_on='countryCode', right_on='iso2').drop(columns='iso2')\n",
    "# # lien = lien.merge(cc_code, how='left', left_on='proposal_countryCode', right_on='iso2').drop(columns='iso2')\n",
    "# # lien = (lien.rename(columns={'iso3_x':'country_code_mapping', 'iso3_y':'proposal_country_code_mapping'})\n",
    "# #         .drop(columns=['countryCode', 'proposal_countryCode']))\n",
    "# # lien.to_pickle(f\"{PATH_CLEAN}lien.pkl\")\n",
    "\n",
    "# # entities = (entities.merge(cc_code, how='left', left_on='countryCode', right_on='iso2').drop(columns='iso2')\n",
    "# #             .rename(columns={'iso3':'country_code_mapping'}))\n",
    "\n",
    "# # part = part.merge(cc_code, how='left', left_on='countryCode', right_on='iso2').drop(columns='iso2').rename(columns={'iso3':'country_code_mapping'})\n",
    "# # app1 = app1.merge(cc_code, how='left', left_on='countryCode', right_on='iso2').drop(columns='iso2').rename(columns={'iso3':'country_code_mapping'})\n",
    "\n",
    "# # #ENTITIES +LIEN\n",
    "# # # entities = entities_cleaning(entities)\n",
    "# # entities.to_pickle(f\"{PATH_SOURCE}entities.pkl\")\n",
    "# # entities_single = entities_single_create(entities, lien)\n",
    "# # entities_info = entities_info_create(entities_single, lien)\n",
    "\n",
    "# # ### step3\n",
    "\n",
    "# # # ##################################\n",
    "# # # # nouvelle actualisation ; à executer UNE FOIS\n",
    "# # # ref_source = ref_source_load('ref')\n",
    "# # # result, check_id_liste, identification = first_update(ref_source, entities_info, countries)\n",
    "\n",
    "# # # # vérifier dans excel les nouveaux ID PATH_WORK/_check_id_result.xlsx\n",
    "# # # IDchecking_results(result, check_id_liste, identification)\n",
    "\n",
    "# # # id_verified = ID_resultChecked()\n",
    "# # # new_ref_source(id_verified, ref_source, extractDate, part, app1, entities_single, countries)\n",
    "\n",
    "# # # ########################################################################################################\n",
    "\n",
    "# # # chargement du nouveau ref_source\n",
    "# # ref_source = ref_source_load('ref')\n",
    "# # ref, genPic_to_new = ref_source_2d_select(ref_source, 'HE')\n",
    "# # entities_tmp = entities_tmp_create(entities_info, countries, ref)\n",
    "# # print(f\"size entities_tmp: {len(entities_tmp)}\")\n",
    "# # entities_tmp = entities_for_merge(entities_tmp)\n",
    "\n",
    "# # ### Executer uniquement si besoin\n",
    "# # # lid_source, unknow_list = ID_entities_list(ref_source)\n",
    "# # # ror = ror_getRefInfo(lid_source)\n",
    "# # # siren_siret = get_siret_siege(lid_source)\n",
    "# # # paysage_id = ID_to_IDpaysage(lid_source, siren_siret)\n",
    "# # # paysage, paysage_mires = paysage_getRefInfo(lid_source, siren_siret, paysage_old=None)\n",
    "# # # paysage_category = IDpaysage_category(paysage)\n",
    "# # # sirene = get_sirene(lid_source, sirene_old=None)\n",
    "\n",
    "# # #############################################################################################################\n",
    "\n",
    "\n",
    "# # ### merge entities_tmp + referentiel\n",
    "# # # ROR\n",
    "# # ### si besoin de charger ror pickle\n",
    "# # ror = pd.read_pickle(f\"{PATH_REF}ror_df.pkl\")\n",
    "# # entities_tmp = merge_ror(entities_tmp, ror, countries)\n",
    "\n",
    "# # # PAYSAGE\n",
    "# # ### si besoin de charger paysage pickle\n",
    "# # paysage = pd.read_pickle(f\"{PATH_REF}paysage_df.pkl\")\n",
    "# # # paysage_category = IDpaysage_category(paysage)\n",
    "# # paysage_category = pd.read_pickle(f\"{PATH_SOURCE}paysage_category.pkl\")\n",
    "# # cat_filter = category_paysage(paysage_category)\n",
    "# # entities_tmp = merge_paysage(entities_tmp, paysage, cat_filter)\n",
    "\n",
    "# # # SIRENE\n",
    "# # ### si besoin de charger paysage pickle\n",
    "# # sirene = pd.read_pickle(f\"{PATH_REF}sirene_df.pkl\")\n",
    "# # entities_tmp = merge_sirene(entities_tmp, sirene)\n",
    "\n",
    "# # entities_tmp.loc[(~entities_tmp.id.isnull())&(entities_tmp.entities_id.isnull()), 'entities_id'] = entities_tmp.id\n",
    "\n",
    "# # if any(entities_tmp.siren.str.contains(';', na=False)):\n",
    "# #     print(\"ATTENTION faire code pour traiter deux siren différents -> ce qui serait bizarre qu'il y ait 2 siren\")\n",
    "\n",
    "# # # IDENT with '-' : traitement des identifiants avec '-' pour regrouper multi-pic non identifiés\n",
    "# # entities_tmp = IDpic(entities_tmp)\n",
    "# # entities_tmp = entities_tmp.merge(get_source_ID(entities_tmp, 'entities_id'), how='left', on='entities_id')\n",
    "\n",
    "# # ### groupe entreprises\n",
    "# # # groupe = groupe_treatment('groupe_prov', 'groupe')\n",
    "# # ### si besoin de charger groupe \n",
    "# # groupe = pd.read_pickle(f\"{PATH_REF}groupe.pkl\")\n",
    "# # print(f\"taille de entities_tmp avant groupe:{len(entities_tmp)}\")\n",
    "# # entities_tmp = merge_groupe(entities_tmp, groupe)\n",
    "\n",
    "\n",
    "# # entities_tmp = entities_clean(entities_tmp)\n",
    "# # entities_check_null(entities_tmp)\n",
    "\n",
    "# # # traitement catégorie\n",
    "# # entities_tmp = category_woven(entities_tmp, sirene)\n",
    "# # entities_tmp = category_agreg(entities_tmp)\n",
    "# # entities_info = entities_info_add(entities_tmp, entities_info, countries)\n",
    "# # entities_info = cordis_type(entities_info)\n",
    "\n",
    "# # entities_info = fix_countries(entities_info, countries)\n",
    "# # entities_info = mires(entities_info)\n",
    "\n",
    "# # del ref_source\n",
    "\n",
    "# # #check entities with pic_id\n",
    "# # print(\"### check enties fr avec id commençant par pic\")\n",
    "# # pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "# # print(entities_info[(entities_info.country_code=='FRA')&(entities_info.entities_id.str.contains('pic'))][['entities_id', 'entities_name']])\n",
    "\n",
    "# # file_name = f\"{PATH_CLEAN}entities_info_current2.pkl\"\n",
    "# # with open(file_name, 'wb') as file:\n",
    "# #     pd.to_pickle(entities_info, file)\n",
    "\n",
    "# # STEP4 - INDICATEURS\n",
    "\n",
    "# part_step = entities_with_lien(entities_info, lien, genPic_to_new)\n",
    "# proj_no_coord = proj_no_coord(projects)\n",
    "\n",
    "\n",
    "\n",
    "# def applicants_calcul(part_step, app1):\n",
    "#     '''Traitement des subventions proposals -> création calculated_proposal_subv'''\n",
    "#     print(\"\\n### CALCULS applicants\")\n",
    "#     subv_p = (part_step.loc[part_step.inProposal==True, ['project_id', 'generalPic', 'country_code_mapping', 'proposal_orderNumber', 'projNlien']]\n",
    "#             .drop_duplicates()\n",
    "#             .merge(app1[['project_id', 'generalPic', 'country_code_mapping', 'orderNumber', 'role', 'partnerType', 'erc_role', 'requestedGrant']], \n",
    "#                 how='inner',\n",
    "#                 left_on=['project_id', 'generalPic', 'country_code_mapping', 'proposal_orderNumber'],\n",
    "#                 right_on=['project_id', 'generalPic', 'country_code_mapping', 'orderNumber']))\n",
    "\n",
    "#     print(f\"0 - {'{:,.1f}'.format(app1['requestedGrant'].sum())}, {'{:,.1f}'.format(subv_p['requestedGrant'].sum())}\")\n",
    "\n",
    "#     subv_p['calculated_fund'] = np.where(subv_p['projNlien']>1, subv_p['requestedGrant']/subv_p['projNlien'], subv_p['requestedGrant'])\n",
    "#     subv_p.drop(['projNlien', 'orderNumber'], axis=1, inplace=True)\n",
    "\n",
    "#     if len(subv_p)!=len(app1):\n",
    "#         print(f\"1- ATTENTION ! {len(subv_p)-len(app1)} participations perdues entre app1 et subv_p\")\n",
    "\n",
    "#     app_sum = '{:,.1f}'.format(app1['requestedGrant'].sum())\n",
    "    \n",
    "#     if '{:,.1f}'.format(subv_p['requestedGrant'].sum()) == app_sum:\n",
    "#         print(\"2- requests grants = subventions proposals OK\")\n",
    "#     else:\n",
    "#         print(f\"3- ATTENTION ! Ecart subventions proposals -> subv_orig:{app_sum}, après fusion:{'{:,.1f}'.format(subv_p['requestedGrant'].sum())}\")\n",
    "        \n",
    "#     part_prop = (part_step.merge(subv_p, how='inner')[\n",
    "#                 ['project_id',  'generalPic', 'proposal_orderNumber', 'proposal_participant_pic', 'participation_linked', \n",
    "#                 'erc_role', 'cordis_is_sme',  'flag_entreprise', 'groupe_id', 'groupe_name', 'groupe_acronym',\n",
    "#                 'cordis_type_entity_code', 'cordis_type_entity_name_fr', 'cordis_type_entity_name_en', 'cordis_type_entity_acro', \n",
    "#                 'participation_nuts', 'region_1_name', 'region_2_name', 'regional_unit_name',\n",
    "#                 'country_code', 'country_code_mapping', 'extra_joint_organization', 'role', 'partnerType', 'calculated_fund']]\n",
    "#                 .rename(columns={'proposal_orderNumber':'orderNumber', 'proposal_participant_pic':'pic'})\n",
    "#                 .assign(stage='evaluated'))\n",
    "\n",
    "#     if '{:,.1f}'.format(part_prop['calculated_fund'].sum())==app_sum:\n",
    "#         print(\"4- Etape part_prop/subv_p -> calculated_proposal_subv OK\")\n",
    "#     else:\n",
    "#         print(f\"5- ATTENTION ! bien vérifier le volume de calculated_proposal_subv dans PARTICIPATION FINALE :{'{:,.1f}'.format(part_prop['calculated_fund'].sum())}, subv_orig:{app_sum}\")\n",
    "\n",
    "#     print(f\"- size part_prop: {len(part_prop)}\")\n",
    "#     return part_prop\n",
    "\n",
    "\n",
    "\n",
    "# part_prop = applicants_calcul(part_step, app1)\n",
    "# part_proj = participants_calcul(part_step, part)\n",
    "# participation = participations_complete(part_prop, part_proj, proj_no_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def entities_single_create(df, lien):\n",
    "\n",
    "#     print(\"\\n### ENTITIES SINGLE\")\n",
    "#     entities_single=df.groupby(['generalPic', 'country_code_mapping']).head(1)\n",
    "#     print(f\"- size entities after one selection pic+cc: {len(entities_single)}\")\n",
    "\n",
    "#     print(f\"\\n- {entities_single.generalState.value_counts()}\")\n",
    "#     if (entities_single.generalPic.nunique())==(lien.generalPic.nunique()):\n",
    "#         print(f\"\\n1 - nombre de pics OK\")\n",
    "#     #si pas le m^me nombre de pics entre lien et entities\n",
    "#     elif len(set(lien.generalPic.unique()))>len(set(df.generalPic.unique())):\n",
    "#         pic_lien=list(set(lien.generalPic.unique()) - set(df.generalPic.unique()))\n",
    "#         print(f\"\\n2 - pic_lien absent de entities_single {pic_lien}; faire code\")\n",
    "\n",
    "#     tmp=entities_single.groupby(['generalPic', 'country_code_mapping']).filter(lambda x: x['generalPic'].count() > 1.)\n",
    "#     if not tmp.empty:\n",
    "#         print(f\"1 - ATTENTION doublon generalPic revoir code ci-dessous si besoin\")\n",
    "           \n",
    "#     print(f\"- size entities_single:{len(entities_single)}\")\n",
    "#     return entities_single\n",
    "\n",
    "\n",
    "# entities_single = entities_single_create(entities, lien)\n",
    "entities_single = entities_single_create(entities, lien)\n",
    "\n",
    "def entities_info_create(entities_single, lien):\n",
    "    print(\"\\n### ENTITIES INFO\")\n",
    "    entities_info = (entities_single\n",
    "                     .drop(['pic', 'cedex', 'countryCode_y','lastUpdateDate'], axis=1)\n",
    "                     .drop_duplicates())\n",
    "\n",
    "    if len(entities_info[['generalPic', 'country_code_mapping']].drop_duplicates())!=len(lien[['generalPic', 'country_code_mapping']].drop_duplicates()):\n",
    "        print(f\"1- ATTENTION ! size genPic+cc -> entities_info : \\n{len(entities_info[['generalPic', 'country_code_mapping']].drop_duplicates())},  lien:{len(lien[['generalPic', 'country_code_mapping']].drop_duplicates())}\")\n",
    "    else:\n",
    "        pass\n",
    "    print(f\"- size entities_info: {len(entities_info)}\")\n",
    "    return entities_info\n",
    "entities_info = entities_info_create(entities_single, lien)\n",
    "entities_info.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       a  b\n",
       "index      \n",
       "0      1  2\n",
       "1      2  2\n",
       "2      3  3\n",
       "3      4  4\n",
       "5      4  5\n",
       "5      5  5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'a':[1,2,2,3,4,4,5], 'b':[2,2,2,3,4,5,5]}, index=[0,1,1,2,3,5,5])\n",
    "df.reset_index().drop_duplicates(subset=['index','b', 'a']).set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_source = ref_source_load('ref')\n",
    "# ref, genPic_to_new = ref_source_2d_select(ref_source, 'HE')\n",
    "\n",
    "def entities_tmp_create(entities_info, countries, ref):\n",
    "    print(\"### create ENTITIES TMP pour ref\")\n",
    "    tab = entities_info.merge(countries[['country_code_mapping', 'country_name_mapping', 'countryCode_parent']], how='left', on='country_code_mapping')\n",
    "    tmp = tab.merge(ref, how='inner', on=['generalPic','country_code_mapping'])\n",
    "    print(f\"- size entities_info before:{len(entities_info)}, size entities_info+ref -> tmp:{len(tmp)}, Pic unique tmp:{len(tmp.generalPic.unique())}\")\n",
    "    # entities only into entities_info\n",
    "    print(\"# missing entities into ref\")\n",
    "    tmp2 = tab.merge(tmp[['generalPic','country_code_mapping']], how='left', on=['generalPic','country_code_mapping'], indicator=True).query('_merge==\"left_only\"').drop(columns=['_merge'])\n",
    "    print(f\"- entities_info en + -> (tmp2): {len(tmp2)}\")\n",
    "    if not tmp2.empty:\n",
    "        # test lien avec ref voire si un identifiant seulement sur le generalPic\n",
    "        tmp2 = tmp2.merge(ref.drop(columns='country_code_mapping').drop_duplicates(), how='inner', on='generalPic')\n",
    "        print(f\"- size lien tmp2 with ref: {len(tmp2)}\")\n",
    "        ## add tmp2 to tmp\n",
    "        tmp1 = pd.concat([tmp, tmp2], ignore_index=True)\n",
    "        # entities_info without id\n",
    "        tmp1 = tab.merge(tmp1[['generalPic','country_code_mapping']], how='left',on=['generalPic','country_code_mapping'], indicator=True).query('_merge==\"left_only\"').drop(columns=['_merge'])\n",
    "        print(f\"- size entities_info without id -> tmp1: {len(tmp1)}\")\n",
    "        tmp = pd.concat([tmp1, tmp], ignore_index=True)\n",
    "\n",
    "    if (len(tmp))!=(len(entities_info)):\n",
    "        print(f\"1 - ATTENTION!!! size result {len(tmp)} diff size entities_info {len(entities_info)}\")\n",
    "    print(f\"- End size entities_tmp {len(tmp)}\")\n",
    "    return tmp\n",
    "entities_tmp = entities_tmp_create(entities_info, countries, ref)\n",
    "print(f\"size entities_tmp: {len(entities_tmp)}\")\n",
    "entities_tmp = entities_for_merge(entities_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ror = pd.read_pickle(f\"{PATH_REF}ror_df.pkl\")\n",
    "def merge_ror(entities_tmp, ror, countries):\n",
    "    print(\"### merge ROR\")\n",
    "    ccode=json.load(open(\"data_files/countryCode_match.json\"))\n",
    "    for i in ccode:\n",
    "        for k,v in i.items():\n",
    "            ror.loc[ror.country_code==k, 'country_code'] = v\n",
    "            ror.loc[ror.country_code==k, 'country_code'] = v\n",
    "    ror = (ror\n",
    "           .merge(countries[['countryCode', 'country_code_mapping']], \n",
    "                  how='left', left_on='country_code', right_on='countryCode')\n",
    "            .drop(columns=['countryCode', 'country_code']))\n",
    "\n",
    "    entities_tmp = (entities_tmp\n",
    "                    .merge(ror.drop(columns='country_code_mapping'), \n",
    "                           how='left', left_on=['id_extend'], right_on=['id_source'])\n",
    "                    .drop(columns='id_source')\n",
    "                    .drop_duplicates())\n",
    "    print(f\"- End size entities_tmp+ror_info: {len(entities_tmp)}\")\n",
    "    if any(entities_tmp.groupby('generalPic')['generalPic'].transform('count')>1):\n",
    "        entities_tmp[entities_tmp.groupby('generalPic')['generalPic'].transform('count')>1]\n",
    "    return entities_tmp\n",
    "ror\n",
    "entities_tmp = merge_ror(entities_tmp, ror, countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paysage = pd.read_pickle(f\"{PATH_REF}paysage_df.pkl\")\n",
    "# paysage_category = IDpaysage_category(paysage)\n",
    "paysage_category = pd.read_pickle(f\"{PATH_SOURCE}paysage_category.pkl\")\n",
    "cat_filter = category_paysage(paysage_category)\n",
    "entities_tmp = merge_paysage(entities_tmp, paysage, cat_filter)\n",
    "sirene = pd.read_pickle(f\"{PATH_REF}sirene_df.pkl\")\n",
    "entities_tmp = merge_sirene(entities_tmp, sirene)\n",
    "\n",
    "entities_tmp.loc[(~entities_tmp.id.isnull())&(entities_tmp.entities_id.isnull()), 'entities_id'] = entities_tmp.id\n",
    "\n",
    "if any(entities_tmp.siren.str.contains(';', na=False)):\n",
    "    print(\"ATTENTION faire code pour traiter deux siren différents -> ce qui serait bizarre qu'il y ait 2 siren\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_tmp = IDpic(entities_tmp)\n",
    "entities_tmp = entities_tmp.merge(get_source_ID(entities_tmp, 'entities_id'), how='left', on='entities_id')\n",
    "\n",
    "### groupe entreprises\n",
    "# groupe = groupe_treatment('groupe_prov', 'groupe')\n",
    "### si besoin de charger groupe \n",
    "groupe = pd.read_pickle(f\"{PATH_REF}groupe.pkl\")\n",
    "print(f\"taille de entities_tmp avant groupe:{len(entities_tmp)}\")\n",
    "entities_tmp = merge_groupe(entities_tmp, groupe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_tmp = entities_clean(entities_tmp)\n",
    "entities_check_null(entities_tmp)\n",
    "\n",
    "# traitement catégorie\n",
    "entities_tmp = category_woven(entities_tmp, sirene)\n",
    "entities_tmp = category_agreg(entities_tmp)\n",
    "def entities_info_add(entities_tmp, entities_info, countries):\n",
    "    print(\"\\n### entities_info + entities_tmp\")\n",
    "        #ajout des infos country à participants_info\n",
    "    entities_info = (entities_info\n",
    "                    .merge(countries[['country_code_mapping', 'country_name_mapping', 'country_code']], how='left', on='country_code_mapping'))\n",
    "        \n",
    "    entities_info = (entities_info\n",
    "        .merge(entities_tmp[\n",
    "            ['generalPic', 'id', 'ZONAGE', 'id_m', 'siren', 'country_code_mapping',\n",
    "            'id_secondaire', 'entities_id',  'entities_name', 'entities_acronym', \n",
    "            'insee_cat_code', 'insee_cat_name',  'category_agregation',\n",
    "            'paysage_category', 'flag_entreprise', \n",
    "            'ror_category', 'category_woven', 'source_id', 'sector',  \n",
    "            'siret_closeDate', 'cat_an',\n",
    "            'groupe_name','groupe_acronym', 'groupe_id', 'groupe_sector']],\n",
    "        how='left', on=['generalPic', 'country_code_mapping'])\n",
    "        .drop(columns=['legalName', 'businessName']))\n",
    "    print(f\"- size entities_info + entities_tmp: {len(entities_info)}\")\n",
    "    return entities_info\n",
    "entities_info = entities_info_add(entities_tmp, entities_info, countries)\n",
    "entities_info = cordis_type(entities_info)\n",
    "\n",
    "entities_info = fix_countries(entities_info, countries)\n",
    "entities_info = mires(entities_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcri-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
